{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from easynmt import EasyNMT\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "dataset_path = \"../storage/dataset\"\n",
    "dataset_csv = \"../storage/dataset.csv\"\n",
    "translated = \"../storage/translated-description\"\n",
    "generated = \"../storage/generated-description\"\n",
    "translated_random_split = \"../storage/translated-description_random-split\"\n",
    "generated_random_split = \"../storage/generated-description_random-split\"\n",
    "translated_category_split = \"../storage/translated-description_category-split\"\n",
    "generated_category_split = \"../storage/generated-description_category-split\""
   ],
   "id": "1df7dbd793023f36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = EasyNMT(\"opus-mt\")\n",
    "\n",
    "\"\"\"SDXL and its fine-tuning works better with english texts. It makes sense to translate the german descriptions into english.\"\"\"\n",
    "def translate_image_description():\n",
    "    total_txt_files = sum(\n",
    "        1 for _, _, files in os.walk(dataset_path) for file in files if file.lower().endswith(\".txt\")\n",
    "    )\n",
    "    \n",
    "    progress_bar = tqdm(total=total_txt_files, desc=\"description translated\", unit=\"description\")\n",
    "\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".txt\"):\n",
    "                input_file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, dataset_path)\n",
    "                output_file_dir = os.path.join(translated, relative_path)\n",
    "                output_file_path = os.path.join(output_file_dir, file)\n",
    "\n",
    "                os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "                with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                translated_content = model.translate(content, target_lang=\"en\")\n",
    "\n",
    "                with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(translated_content)\n",
    "                    \n",
    "                progress_bar.update()\n",
    "    progress_bar.close()\n",
    "                \n",
    "\n",
    "translate_image_description()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_caption(text):\n",
    "    # LLaVA adds start and end flags for descriptions\n",
    "    text = re.sub(r\"\\[INST\\].*?\\[/INST\\]\", \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # max new tokens is set to 70 - sometimes the model stops with unfinished sentences\n",
    "    text = re.sub(r\"[^\\.]*$\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "image_captioning_model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\"cuda\")\n",
    "\n",
    "translation_model = EasyNMT(\"opus-mt\")\n",
    "\n",
    "\"\"\"LLaVA can be used to create additional image description and compare those with translated original description.\"\"\"\n",
    "def generate_llava_description():\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        total_elements = len(dirs)\n",
    "        progress_bar = tqdm(total=total_elements, desc=\"description generated\", unit=\"description\")\n",
    "        for dir in dirs:\n",
    "            input_file_path = os.path.join(root, f\"{dir}/{dir}.png\")\n",
    "            output_file_dir = os.path.join(generated, dir)\n",
    "            output_file_path = os.path.join(output_file_dir, f\"{dir}.txt\")\n",
    "            \n",
    "            # translate the title to inject additional context in LLaVA\n",
    "            title = translation_model.translate(dir, source_lang=\"de\", target_lang=\"en\") \n",
    "\n",
    "            os.makedirs(output_file_dir, exist_ok=True)\n",
    "            \n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": f\"What is shown in this image? The title is {title}.\"},\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "            prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "            inputs = processor(prompt, Image.open(input_file_path), return_tensors=\"pt\").to(\"cuda:0\")\n",
    "            \n",
    "            output = image_captioning_model.generate(**inputs, max_new_tokens=70)\n",
    "            \n",
    "            image_caption = clean_caption(processor.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(image_caption)\n",
    "                progress_bar.update()\n",
    "                \n",
    "        progress_bar.close()\n",
    "\n",
    "generate_llava_description()"
   ],
   "id": "f4078afcfc7591e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Images with at least 512 pixels can produce better fine-tuning results\"\"\"\n",
    "def upscale_images(output_dir):\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".png\"):\n",
    "                input_file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(root, dataset_path)\n",
    "                output_file_dir = os.path.join(output_dir, relative_path)\n",
    "                output_file_path = os.path.join(output_file_dir, file)\n",
    "\n",
    "                os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "                low_res_img = Image.open(input_file_path).convert(\"RGB\")\n",
    "                \n",
    "                original_width, original_height = low_res_img.size\n",
    "                \n",
    "                # keep aspect ratio\n",
    "                if original_width > original_height:\n",
    "                    new_width = 512\n",
    "                    new_height = int((512 / original_width) * original_height)\n",
    "                else:\n",
    "                    new_height = 512\n",
    "                    new_width = int((512 / original_height) * original_width)\n",
    "                \n",
    "                high_res_img = low_res_img.resize((new_width, new_height))\n",
    "\n",
    "                high_res_img.save(output_file_path)\n",
    "\n",
    "\n",
    "for dir in [translated, generated]:\n",
    "    upscale_images(dir)"
   ],
   "id": "b5bc2b3da51dc442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"The traditional way to split training and test datasets is to create random partitions\"\"\"\n",
    "def random_dataset_split(folder_path, new_folder_name, test_size=0.2, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    os.makedirs(new_folder_name, exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(new_folder_name, \"train\")\n",
    "    test_dir = os.path.join(new_folder_name, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    all_folders = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    random.shuffle(all_folders)\n",
    "    num_test_folders = int(len(all_folders) * test_size)\n",
    "\n",
    "    test_folders = all_folders[:num_test_folders]\n",
    "    train_folders = all_folders[num_test_folders:]\n",
    "\n",
    "    for folder in test_folders:\n",
    "        src_path = os.path.join(folder_path, folder)\n",
    "        dest_path = os.path.join(test_dir, folder)\n",
    "        shutil.copytree(src_path, dest_path)\n",
    "\n",
    "    for folder in train_folders:\n",
    "        src_path = os.path.join(folder_path, folder)\n",
    "        dest_path = os.path.join(train_dir, folder)\n",
    "        shutil.copytree(src_path, dest_path)\n",
    "\n",
    "\n",
    "for dir in [translated, generated]:\n",
    "    random_dataset_split(dir, f\"{dir}_random-split\", test_size=0.2, seed=42)"
   ],
   "id": "4b4d7a89a970aae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Since the dataset from LAG Selbsthilfe is highly unbalanced and biased you can also create a test dataset with samples from the category \"Begriffe only\" to reduce the bias a little bit. \"\"\"\n",
    "def split_dataset_by_category(csv_path, category, source_folder_path, target_folder_path):\n",
    "    train_dir = os.path.join(target_folder_path, \"train\")\n",
    "    test_dir = os.path.join(target_folder_path, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    category_df = df[df[\"category\"] == category]\n",
    "    category_titles = category_df[\"title\"].tolist()\n",
    "\n",
    "    all_folders = [os.path.join(source_folder_path, d) for d in os.listdir(source_folder_path) if os.path.isdir(os.path.join(source_folder_path, d))]\n",
    "\n",
    "    for folder in all_folders:\n",
    "        folder_name = os.path.basename(folder)\n",
    "        if folder_name in category_titles:\n",
    "            dest_path = os.path.join(test_dir, folder_name)\n",
    "        else:\n",
    "            dest_path = os.path.join(train_dir, folder_name)\n",
    "\n",
    "        if not os.path.exists(dest_path):\n",
    "            shutil.copytree(folder, dest_path)\n",
    "\n",
    "for dir in [translated, generated]:\n",
    "    split_dataset_by_category(\n",
    "    csv_path=dataset_csv,\n",
    "    category=\"Begriffe\",\n",
    "    source_folder_path=dir,\n",
    "    target_folder_path=f\"{dir}_category-split\"\n",
    ")"
   ],
   "id": "933f0194b8fc2262",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Fine-Tuning with sd-scripts expects a folder structure with text- and image files in one shared directory. sd-scripts gets some hyperparameters e.g. the number of repeats per image from the training image folder name\"\"\"\n",
    "def format_training_folder(\n",
    "        dataset_dir,\n",
    "        suffix=\" leichte sprache style\",\n",
    "        number_repeats=20,\n",
    "        instance_prompt=\"leichte sprache style\",\n",
    "        class_prompt=\"style\"\n",
    "):\n",
    "    image_extensions = {\".png\"}\n",
    "    text_extension = \".txt\"\n",
    "    # sd gets the number of repeats from dir prefix\n",
    "    output_dir = f\"{dataset_dir}/fine-tuning/{int(number_repeats)}_{instance_prompt} {class_prompt}\"\n",
    "    lora_dir = f\"{dataset_dir}/loras\"\n",
    "    log_dir = f\"{dataset_dir}/logs\"\n",
    "    \n",
    "    # create dir where lora-weights can be stored\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # create dir where lora-weights can be stored\n",
    "    os.makedirs(lora_dir, exist_ok=True)\n",
    "    \n",
    "    # create dir where training logs can be stored\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    for root, _, files in os.walk(f\"{dataset_dir}/train\"):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1].lower() in image_extensions:\n",
    "                image_path = os.path.join(root, file)\n",
    "                description_file = os.path.splitext(file)[0] + text_extension\n",
    "                description_path = os.path.join(root, description_file)\n",
    "\n",
    "                if os.path.isfile(description_path):\n",
    "                    shutil.copy2(image_path, output_dir)\n",
    "                    target_text_path = os.path.join(output_dir, os.path.basename(description_path))\n",
    "                    \n",
    "                    with open(description_path, \"r\", encoding=\"utf-8\") as src_file:\n",
    "                        description = src_file.read().strip()\n",
    "                        \n",
    "                        # suffix to link prompt to fine-tuning context\n",
    "                        if suffix:\n",
    "                            description += suffix\n",
    "                        with open(target_text_path, \"w\", encoding=\"utf-8\") as dest_file:\n",
    "                            dest_file.write(description)\n",
    "\n",
    "\n",
    "for dir in [translated_category_split, translated_random_split, generated_category_split, generated_random_split]:\n",
    "    format_training_folder(dir)"
   ],
   "id": "889e05ee88238cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resize_image(image_path, size):\n",
    "    with Image.open(image_path) as img:\n",
    "        img_resized = img.resize((size, size))\n",
    "        img_resized.save(image_path)\n",
    "\n",
    "\"\"\"For Evaluation with FID reference and generated images must have the same resolution\"\"\"\n",
    "def format_test_folder(dir):\n",
    "    test_path = f\"{dir}/test\"\n",
    "    new_test_images_path = os.path.join(dir, \"test-images-only\")\n",
    "    \n",
    "    if not os.path.exists(new_test_images_path):\n",
    "        os.makedirs(new_test_images_path)\n",
    "        \n",
    "    for root, dirs, files in os.walk(test_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                dest_path = os.path.join(new_test_images_path, file)\n",
    "\n",
    "                shutil.copy(file_path, dest_path)\n",
    "                \n",
    "                resize_image(dest_path, 512)\n",
    "                \n",
    "for dir in [translated_category_split, translated_random_split, generated_category_split, generated_random_split]:\n",
    "    format_test_folder(dir)"
   ],
   "id": "604e0df3e32b88a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# optional: you can use the four dataset dirs for fine-tuning. Some dirs are not necessary anymore.\n",
    "for dir in [dataset_path, generated, translated]:\n",
    "    shutil.rmtree(dir, ignore_errors=True)"
   ],
   "id": "fd613e904db7174a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
